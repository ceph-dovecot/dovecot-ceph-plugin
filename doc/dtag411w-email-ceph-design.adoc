= Storing Email on Ceph: Design Alternatives of librmb and Dovecot Integration
Peter Mauritius <p.mauritius@tallence.com>
v1.3.0, 2017-03-27: Hybrid Model with CephFS is preferred now.
:icons: font
:toc:
:toclevels: 2
:numbered:
:pdf-page-size: A4
:docinfo: shared,private-footer
:blockdiag: /usr/local/bin/blockdiag

[NOTE]
.This document is under active development and discussion!
If you find errors or omissions in this document, please don’t hesitate to mail the author. We also encourage you to ask questions and discuss any aspects of the project on our Slack channel. New contributors are always welcome!

<<<
[colophon]
== Legal Notice
Project Id:: DTAG411w
Order:: 936 / 4200013130

All information in this document has been compiled in good faith and with due care. The contents has been checked through as well. Nevertheless, errors or omissions can not be excluded. The information and illustrations contained in this document are not associated with any obligation or guarantee of any kind. Data used in examples herein are fictitious unless otherwise noted. All trade names, logos, names, etc. are used without guarantee of free usability and may be particularly protected.

All rights, including reproduction, translation and storage into electronic systems, reserved, unless appropriate use agreement has been concluded. This document and the information contained therein may only be disclosed with the express consent of the third party Tallence AG.

Copyright (C) 2016, 2017 http://www.tallence.com[Tallence AG], Herrenstr. 26, D-76133 Karlsruhe,

<<<
[abstract]
[preamble]
== Abstract
Deutsche Telekom currently runs two projects, both looking for efficiently storing mails within a Dovecot email server environment.

. USD4e (Unlimited Storage Duration for eMail), phase 3
. PAN-IP-eMail

During the design session/workshop of September 1st in Darmstadt multiple solutions for storing email data on Ceph were discussed and looked at.

* Using Ceph's native filesystem: Ceph Filesystem
* Storing email via the S3-compatible Ceph Object Gateway
* Writing a custom library, _librmb_ (librados mailbox)

This document describes the design variants of a solution that stores emails on Ceph using the library librados mailbox (_librmb_) and an integration into Dovecot. The solution to implement must still be chosen.

*Summary:* _librmb_ will be *_faster_*, *_cheaper_* and *_easier_* to scale than any existing solution for larger installations.

<<<
== Overview
=== Ceph Object Storage
Ceph is a fully Open Source distributed storage system. It is a software based solution for distributed storage of objects.

Internally Ceph only thinks in objects. This is done by the heart of Ceph: RADOS (Reliable Autonomous Distributed Object Store).

On top of RADOS these components are available making it a Unified storage solution:

*	Ceph Object Gateway
*	Ceph Block Device (RBD)
*	Ceph File System (CephFS)

All these applications are built on top of *librados*, a _userspace library_ which is the client for Ceph.

[ditaa, "ceph-stack", round-corners=true, separation=false]
----
                +-----+             +-------+           +-----+             +------+
                | APP |             |HOST/VM|           | APP |             |CLIENT|
                +--+--+             +---+---+           +--+--+             +---+--+
                   |                    |                  |                    |
                   |                    |                  |                    |
                   |                    v                  v                    v
                   |          +-------------------+-------------------+-------------------+
                   |          | CEPH BLOCK DEVICE |CEPH OBJECT GATEWAY| CEPH FILE SYSTEM  |
                   |          |                   |                   |                   |
                   v          | A reliable and    | A bucket+based    | A POSIX-compliant |
         +--------------------+ fully-distributed | REST gateway,     | distrbuted file   |
         | LIBRADOS           | block device with | compatible with   | system, with a    |
         |                    | a Linux kernel    | S3 and Swift      | Linux kernel      |
         | A library allowing | client and        |                   | client and support|
         | apps to directly   | QEMU/KVM driver   |                   | for FUSE          |
         | access RADOS       |                   |                   |                   |
         | with support for C,|                   |                   |                   |
         | C✛✛, Java, Python, |               cEEE|               cDDD|               cEEE|
         | Ruby and PHP       +-------------------+-------------------+-------------------+
       +-+                                                                                +-+
       | |                                                                            c9DE| |
       | +--------------------------------------------------------------------------------+ |
       |                                                                                    |
       | RADOS                                                                              |
       |                                                                                    |
       | A software based, reliable, autonomous, distributed object store comprised of      |
       | self healing, self managing, intelligent storage nodes and lightweight monitors.   |
       |                                                                                cF87|
       +------------------------------------------------------------------------------------+
----

Once an object is stored, Ceph takes full responsibility for that object to make sure it is safe and stays consistent.

The maximum performance of Ceph is achieved by using parallel I/O. All clients directly communicate with the machines storing the data.

<<<
=== Ceph File System
One of the options being looked at is using Ceph File System (CephFS).
This is a POSIX compliant shared filesystem like NFS.
Using this filesystem mails could be stored on Ceph like they would be with other shared file systems like NFS, as regular files.

*Advantages*

*	Low deployment time. Replace NFS by CephFS.
*	Scaling: CephFS scales when the Ceph cluster scales, no sharding required.

*Disadvantages*

*	POSIX layer adds additional complexity
*	CephFS requires additional servers called the Ceph MetaData Server (MDS)
**	Additional daemon to maintain
**	Additional servers to run
**	Additional component which can fail
*	Dovecot heavily uses file locking which is heavy for the Ceph MDS
*	Scalability limited by Ceph MDS
*	Additional storage I/O required due to translation of POSIX semantics to objects

Recent developments around CephFS help to overcome the listed disadvantaged. Especially the MDS made a leap forward by supporting

* starting with the Luminous release, http://docs.ceph.com/docs/master/cephfs/multimds/[multiple active MDS servers] per cluster sharing the workload among each other.
* that MDS can keep the contents of the metadata (inodes, directories, etc) in-memory making it very fast.

<<<
=== librmb
The proposal which came up, is to develop the library librados mailbox (_librmb_).
This library will directly link to librados and abstract mails on top of RADOS objects. This allows us to store about 80% of the volume needed for a Dovecot cluster directly in Ceph objects. The indexes and metadata for flags, mailboxes and mailbox lists will be managed by Dovecots lib-index on shared CephFS storage.

This hybrid model is the best combination of the Dovecot and Ceph capabilities. The librados for direct and fast storage of mails combined with the original Dovecot index management, which is the foundation for the flexibility of Dovecot.

[ditaa, "librmb-stack", round-corners=true, separation=false]
----
                +-----+             +-------+                               +------+             +-------+
                | APP |             |  MUA  |                               |CLIENT|             |HOST/VM|
                +--+--+             +---+---+                               +---+--+             +---+---+
                   |                    |                                       |                    |
                   |                    |                                       |                    |
                   |                    v                                       |                    |
                   |          +---------------------------------------+         |                    |
                   |          | DOVECOT                               |         |                    |
                   |          |                                       |         v                    v
                   |          +-------------------+-------------------+-------------------+-------------------+
                   |          | LIBRMB            | CEPH FILE SYSTEM  |CEPH OBJECT GATEWAY| CEPH BLOCK DEVICE |
                   |          |                   |                   |                   |                   |
                   v          | Email             | A POSIX-compliant | A bucket+based    | A reliable and    |
         +--------------------+ abstraction on top| distrbuted file   | REST gateway,     | fully-distributed |
         | LIBRADOS           | of librados       | system, with a    | compatible with   | block device with |
         |                    | with a Dovecot    | Linux kernel      | S3 and Swift      | a Linux kernel    |
         | A library allowing | storage layer     | client and support|                   | client and        |
         | apps to directly   |                   | for FUSE          |                   | client and        |
         | access RADOS       |                   |                   |                   | QEMU/KVM driver   |
         | with support for C,|                   |                   |                   |                   |
         | C✛✛, Java, Python, |               cF84|               cDDD|               cEEE|               cEEE|
         | Ruby and PHP       +-------------------+-------------------+-------------------+-------------------+
       +-+                                                                                                    +-+
       | |                                                                                                c9DE| |
       | +----------------------------------------------------------------------------------------------------+ |
       |                                                                                                        |
       | RADOS                                                                                                  |
       |                                                                                                        |
       | A software based, reliable, autonomous, distributed object store comprised of                          |
       | self healing, self managing, intelligent storage nodes and lightweight monitors.                       |
       |                                                                                                    cF87|
       +--------------------------------------------------------------------------------------------------------+
----

Ceph provides two ways for storing data:

*	Replication (3x by default)
*	Erasure Coding

With 3x replication storing a 1M object will consume 3M of storage.
With EC this 1M will use between 1.5M and 1.8M of storage.

The downside for EC however is that EC objects can't be partially overwritten due to the nature of EC.
When storing mails as Objects they are never changed afterwards however. Emails are Write Once, Read Many (WORM) and can take advantage of EC.

CephFS for index management, on the other hand, is best placed in an SSD-based pool with reolation. Here, speed is more important than space consumption.

*Advantages*

*	Easy scaling of email storage by scaling Ceph
**	_librmb_ talks directly to RADOS without additional layers and/or daemons
*	Lower latency by eliminating additional RGW or CephFS layers
**	No additional to layers and/or daemons
*	Allows for storing email on Erasure Coded pools
**	Saves large amount of storage
*	Less I/O on backing storage due to the lack of overhead from additional layers
*	Possibility to directly interface with _librmb_ to access email without IMAP/POP3 via Dovecot.
    This allows for example high performance REST APIs for WebMail access.

*Disadvantages*

*	Requires development of _librmb_
**	~ 9 months of development and testing

=== Dovecot Integration
Dovecot needs to be expanded to include a new storage layer, which hooks into _librmb_ and stores mails on Ceph.

The library _librmb_ will abstract mails on top of RADOS. Using the semantics of RADOS, emails can be stored directly as objects while index and metadata will be kept on CephFS based shared storage.

[ditaa, "librmb-dovecot", round-corners=true, separation=false]
----
   +-----------------+
   | Mail User Agent |
   +--------+--------+
            |
            | IMAP/POP
            v
 +----------+------------+
 |Ceph Client            |
 | +-------------------+ |
 | |      Dovecot      | |
 | |                   | |
 | +-------------------+ |
 | |rbox storage plugin| |
 | |cF84               | |
 | +----------+--------+ |
 | |   librmb | cephFS | |
 | |cF84      |   c9DE | |
 | +----------+--------+ |
 | |     librados  c9DE| |
 | +-------------------+ |
 |                   cEEE|
 +----------+------------+
            |
            | Ceph Storage Cluster Protocol
            v
 +----------+-----------+
 | RADOS                |
 |                  cF87|
 +----------------------+
----

<<<
== Requirements

The high level goals of the project are as follows:

* Allow a mail server to run solely on Ceph without any local file system for volume data.
* Volume data is defined as any data scaled by number of objects and users
** Mails
** Mailboxes
** Metadata
** Index data
** Key / value data

What subjects are not addressed by this project:

* User data and credentials storage are no subject for the librmb, because we are targeting huge installations that have usually a solution in place for this data.
* Full text indexes. The are a lot of FTS solutions already in place that work outside the mail storage.

=== Anatomy of Dovecot internal APIs

Because Dovecot is a major mail server with an extraordinary market share and the first integration target for librmb, we will have a closer look at the internal storage related APIs of Dovecot.

Some of the following information has been extracted from the http://wiki.dovecot.org/FrontPage[Dovecot Wiki] or the Dovecot source code.

==== lib-storage
The lib-storage is the internal API for the storage and administration of mails and mailboxes. All existing storage formats are implemented in this library. The lib-storage has an internal interface to the respective storage formats. This interface can also be provided by Dovecot plugins. It is therefore possible to develop your own memory formats and use them in Dovecot. Of course you have to move in the frame set by Dovecot.

The building blocks for this are

Mail storage:: is used to access mails in a specific location with a specific mailbox format. Multiple namespaces can point to the same storage. A single namespace may in future (but not currently) point to multiple storage locations (e.g. a mixed mbox and Maildir directory).

Mailbox list:: The purpose of mailbox list is to manage mailbox storage name <--> physical directory path mapping. Its most important functions are:
* listing existing mailboxes,
* creating directories for new mailboxes (but not the mailboxes themselves, that's storage's job),
* deleting mailboxes,
* renaming mailboxes and
* managing mailbox subscriptions.

Mailbox:: is used to access a specific mailbox in a storage. The most important functions are:
mailbox_get_status()::: to get a summary of mailbox, such as number of messages in it.
Syncing: mailbox_sync_*()::: to synchronize changes from the backend to memory.
Transactions::: mailbox_transaction_*() for transaction handling. All message reads and writes are done in a transaction.
Searching::: mailbox_search_*() is used for searching messages. Even simple operations like "get all messages" go through this API, it'll then simply do "search all".
Saving::: mailbox_save_*() and mailbox_copy() is used for saving/copying new messages to mailbox.

Mail:: is used to access a specific mail in a mailbox. Mails are immutable regarding the RFC822 part. Only flags and keywords can be written.

Virtually all existing storage formats use the Dovecot index and cache files, and essentially differ in the way the actual mail is stored. For some older formats, a part of the index data is stored in the file system.

===== Indexes & Caches

In general Dovecot separates the immutable RFC5322 mail data form the writable index data describing the state of mails and their aggregation in mailboxes. Usually the mailbox formats use index files that store the writable part and reference the mails on a storage. The index files and the cache files are the heart of Dovecots mail storage and allow a fast access for all data that is used to generate the message lists.

If you want to create a new storage format that works without this index management, you would have to re-implement large and performance-relevant parts of the Dovecot.

If the index files are lost only the mails without any status information, keywords, attributes or assignment to mailboxes can be recovered.

The core implementations is provided by lib-index. Like most parts of Dovecot it has been implemented very filesystem minded.

<<<

===== lib-dict

Another general purpose Dovecot API for storing arbitrary key / value data is the lib-dict. As the lib-storage the lib-dict has an internal API that can be provided be plugins. A simple librados based dict drive had already been implemented during the research for this document.

Dictionaries are used by for example quota-dict, passdb & userdb, last-login plugin, METADATA, etc. The dictionaries can be accessed either directly by the mail processes or they can be accessed via dict proxy processes.

Currently supported dict backends are:

* Flat files
* FS (lib-fs wrapper)
* Memcached (ASCII protocol)
* Memcached (Binary protocol)
* Redis
* Proxy
* SQL
** mysql
** pgsql
** sqlite
** cassandra
* LDAP

The dictionary API supports set, delete and increment of key / values using transactions, lookup of single keys and iteration over a set of keys. Special prefixes for the keys separate shared and user specific keys:

shared/:: Keys using this prefix are stored in a namespace shared by all users.
priv/:: Keys using this prefix are stored in a namespace that is specific to the user in session. The usual implementation is to replace _priv_ by some kind of username.

The keys and values are C strings not binary bytes. One exception is the operation _atomic_inc_ which does an atomic increment of the identified value. So the value must be an integer.

The transactions already implemented, do no rollback if one of the operations fails during commit.

<<<

===== lib-fs

The File System API is the internal abstraction for a POSIX filesystem. It can be overridden by plugins to support requirements like caching etc. It is not used for all file system access. Especially the lib-index is using POSIX calls directly.

[quote, Timo Sirainen, http://dovecot.org/pipermail/dovecot/2016-March/103373.html]
Currently the only mail storage backend that uses only lib-fs calls is the commercial object storage plugin. All the other storages use POSIX calls directly. Especially problematic is that lib-index uses POSIX calls also. So one possibility might be that you implement a storage backend that stores index files only locally, but if the user switches between backends the performance will likely be bad because there are no indexes for the user in the new server. I've some plans to change lib-index to use lib-fs calls, but that's not going to happen anytime soon.

The lib-fs is the layer used by Dovecot Pro to plugin object storage into Dovecot. Together with new caching drivers for the files and a new background syncing process for file bundles Dovecot Pro allows the utilization of object stores as mail storage.

==== Dovecot Pro
Dovecot Pro implements object store usage for mail and index data. The object stores are used as dumb blob stores. All data from object stores is cached in the local file system for subsequent access. The RFC5322 mail data is stored in objects. Each mail in one object. The mails are accessed through lib-fs drivers for the supported cloud storage. They are always cached locally. Object stores that do not support high performance listing of objects for iteration purposes are accompanied by a cassandra database or alike.

The index & cache files are stored as a bundle in an object. They are not used directly but cached in the local file system. Changes to the index & cache files are synced back to the object store. The sync is time based or will happen after a certain number of changes.

.Dovecot Pro obox overview
[ditaa, "dovecot-pro", round-corners=true, separation=false]
....
+---------------------------------------------------------------------------+
|                          IMAP4/POP3/LMTP process                          |
++-------------------------------------------------------------------------++
 |                              storage API                                |
 ++-----------------------------------------------------------------------++
  |                             obox backend                          cYEL|
  +---+----------------------------------------------------------------+--+
      |                                     |                          |
      |                                     v                          |
      |               +--------------------------------------------+   |
      |               |                 metacache              cYEL|   |
RFC 5322 Mails        +-------------------------------+------------+   |write index & cache
      |                                               |          ^     |to local storage
      |                                           sync local     |     |
      v                                        index&cache with  |     |
 +------------------------------+                object store    |     |
 |           fs API             |                     |          |     v
 ++----------------------------++   +----------+      |       +--+--------+
  |      fscache backend   cYEL+--->| local    |      |       |   local   |
  +----------------------------+    | storage  |      |       |  storage  |
  |   object store backend cYEL|    | mail cGRE|      |       | cGRE   {s}|
  +----+-----------------------+    | cache {s}|      |       +-----------+
       |                            +----------+      |
       v                                              v
 +------------------------------------------------------------+
 | +-+ +-+ +-+             object store           +-+ +-+ +-+ |
 | +-+ +-+ +-+                 {s} cF87           +-+ +-+ +-+ |
 | RFC5322 objects                        index&cache bundles |
 +------------------------------------------------------------+
....

This design is reusing a lot of code from the files system based storage and tries to combine the performance of locally accessed files with an object store. Especially for object stores with high latency this design will work very well.

The concepts and the implementation are very file system heavy for understandable reasons. The great success of  Dovecot, especially in terms of performance, has a lot to do with the intelligent management of the index data. To throw this management overboard, you have to think very carefully.

But reducing the object store to a dumb blob store might not be a good solution for low latency object stores like Ceph. Also you are forced to store all meta and index data separately from the mail objects, what might not be suitable for systems like Ceph that have a richer data model than simple blobs.

<<<

== Architecture

For the design of the data model we make some assumptions

Low Latency::
  Ceph will fast enough for all mail operations. It is not necessary to cache any data on local disk. This does not mean that local caches on SSD are not helpful or prohibited. But in the first guess we assume that Ceph will be fast enough to work without local caches.
Simple recovery on Index Loss::
  Each mail belongs to a mailbox (see below) as long as their relationship is stored. Like other popular mail storage formats like http://wiki2.dovecot.org/MailboxFormat/dbox[dbox] or http://www.washington.edu/imap/documentation/mixfmt.txt.html[MIX] the integrity of the mailbox relies on the mailbox indexes. If they get lost it will be hard or impossible to reconstruct a mailbox. If this happens the design must allow to collect all mail objects of an user and put them into a newly created Inbox.
Object Name Indirection::
  The visible names of objects (mailboxs, mails etc.) are subject to changes by the user and therefore can not be directly used in the name of the Ceph objects to prevent the copying of objects. The visible names of users, folders etc. are attributes of the responsible Ceph objects of their collections. The technical names will be UUIDs that are prefixed with userId and type.

=== Mail Data Model

A data model for mail is usually quite straight forward. The IMAP RFC or the JMAP specification are a good blueprint. The complexity arises from the fact, that a mail server has to answer very fast to index data requests like listing mails, status queries etc. The data for common responses has to be prepared in advance, without the necessity to visit each mail object.

Here a short summary of the managed objects without any thoughts on optimization.

==== Mails
Mails are the smallest objects managed in this model. Each mail belongs to at least one mailbox, a collection or directory of mails. Mails are stored independently of mailboxes within the users namespace but contain an attribute pointing back to the mailboxes they belong to. This allows move operations relocating a mail from one mailbox to another without copying.

A mail is immutable regarding its RFC5322 content. Only the boolean status flags and the set of mailboxes it is in are writable. This allows for more efficient caching of messages, and gives easier backwards compatibility for servers implementing an IMAP interface to the same data. The status flags are _answered_, _flagged_, _deleted_, _seen_, _draft_ and _recent_.

Mails must be addressed independently of any MUA visible key to prevent copies when mails are moved between mailboxes.

==== Mailboxes
A mailbox represents a named set of emails. This is the primary mechanism for organising messages within an account. It is analogous to a folder in IMAP or a label in other systems. A mailbox owns a name within a users namespace. A mailbox stores a list of references to the mails belonging to a mailbox.

For compatibility with IMAP, a message MUST belong to one or more mailboxes. The message id does not change if the message changes mailboxes.

==== Mailbox List
The mailbox list hold all mailboxes of an account. The visible mailbox names are cached for faster presentation.

==== Object Mapping

Each RFC5322 mail will be an object. Flags and other metadata might be stored in omap key / values.

.Possible Mapping of Mail Objects to Ceph Objects
[ditaa, "rmb-mail", separation=true, shadow=true]
----
 +----------+       +-----------+
 |mail      |------>|object     |
 +-+--------++      +-+---------+-+
   |RFC2822  |------->|byte array |
   +---------+        +-----------+
   |MIME ptr |------->|omap       |
   +---------+        +-----------+
   |metadata |------->|omap       |
   +---------+        +-----------+

 +-----------+       +-----------+
 |mailbox    |------>|object     |
 +-+---------++      +-+---------+-+
   |index     |------->|omap       |
   +----------+        +-----------+
   |flags     |------->|omap       |
   +----------+        +-----------+
   |headers   |------->|omap       |
   +----------+        +-----------+

 +-----------+       +-----------+
 |mailboxlist|------>|object     |
 +-+---------++      +-+---------+-+
   |list      |------->|omap       |
   +----------+        +-----------+
   |attributes|------->|omap       |
   +----------+        +-----------+
   |metadata  |------->|omap       |
   +----------+        +-----------+
----

.Example for Object Names
[ditaa, "rmb-mailbox-layout", separation=false]
----
/mail/%d/%n/-+-> mails/                                  |
             |      |                                    |
             |      +-> 07A228668CA449B5A27DB47E1A224CAF | ①
             |      +-> 49C74568A0A24DD8B0BDBB28DB662FFA | ①
             |      |   ...                              |
             |      +-> 71251FBFAC454032B4BA37CCE6B43E8C | ①
             |                                           |
             +-> mailboxes/                              | ②
                    |                                    |
                    +-> 3C5B706DA45D40D98D9E65C643D0F921 | ③
                    +-> 225A9A35FC5244CD80CCAB2EF6CEC5D8 | ③
                    |   ...                              |
                    +-> ABAF8D271DC94FD1930455BE277BBC52 | ③
                                                         |
----

<1> mail object
<2> mailbox list object
<3> mailbox object

In the above example the placeholders stand for

%d:: domain id
%n:: user id

=== The Hybrid Storage Model

As described above, a hybrid model is to be followed in the first step. The mails are saved directly as RADOS objects. All other data are stored as before in the file system. This applies in particular to the data of the Iib-index of Dovecot. The file system is designed as shared storage based on CephFS.

Based on the code of the storage format http://wiki.dovecot.org/MailboxFormat/Cydir[Cydir] we develop a hybrid storage as Dovecot plugin. The hybrid storage directly uses the librados for storing mails in Ceph objects. One mail is stored in one object. The mail objects are immutable. Basic flags and metadata can be stored in omap key / value.

It provides an implementation of "rbox" for lib-storage. The code for librados access und the code the Dovecot storage API will separated to make testing without Dovecot easier.

To achieve this right from the start, we need to provide an additional, internal API that encapsulates the functions of the _librmb_. The librmb API must be carefully designed to provide everything Dovecot needs for a storage backend, while still being usable outside of Dovecot.

The hybrid storage directly uses the librados based lbrmb for storing mails in Ceph objects. Each mail is stored in one object. The flags and keywords are stored in omap key / values associated with the mail object if this is useful. The index administration is completely managed by the lib-index and ends up in the CephFS volumes.

Because of the way MUAs access the mails, it may be necessary to provide a local cache of mails from Dovecot. The cache can be located in the main memory or on local SSD storage. However, this optimization is optional and will be implemented only if necessary, as first measurements suggest their necessity.

.Hybrid Storage librmb & lib-index, mail cache managed by librmb
[ditaa, "dovecot-ceph-hybrid-libindex-rmb-cache", round-corners=true, separation=false]
....
+---------------------------------------------------------------------------+
|                          IMAP4/POP3/LMTP process                          |
++-------------------------------------------------------------------------++
 |                              storage API                                |
 ++-----------------------------------------------------------------------++
  |                             rbox backend                         cF84 |
  +----------------------------------------------+-------+---------------++
  |               librmb                    cF84 |       | lib-index     |
  ++-------------+-------------------+-----------+       +-------+-------+
   |librados c9DE|                   :                           |
   +-----+-------+                   :                           |
         |                           :                           v
         |                           :                   +---------------+
         |  RFC 5322 Mails           :                   | Linux Kernel  |
         |                           :                   +---------------+
         |                           :                   | libcephfs cDDD|
         |                           :                   +---------------+
         |                           :                   | librados c9DE |
         |                           :                   +---------------+
         |                           :                           |
         v                           v                           v
 +-------------------------+ +----------------+         +-----------------+
 | +-+ +-+ +-+        Ceph | :  local storage |         |  CephFS for  {s}|
 | +-+ +-+ +-+ {s}         | |    RFC5322 cGRE|         |index & metadata |
 | RFC5322 objects     cF87| |     cache   {s}|         |             cF87|
 +-------------------------+ +----------------+         +-----------------+
....

The mail objects and CephFS should be placed in different pools. The mail objects are immutable and require a lot of storage. They would benefit a lot from http://docs.ceph.com/docs/master/architecture/#erasure-coding[erasure coded pool]. The index date required a lot of writing and are placed on an SSD based CephFS.

With the solutions outlined above, Ceph-based solutions for the dictionaries and FTS must also be implemented in order to store the persistent portions of the mailboxes completely in Ceph.

=== Alternative Storage Models

The basic idea of the hybrid storage model is to combine the already optimized Dovecot lib-index with the performance and scalability of CephFS.

But what happens if this combination does not work? How do we get the index data into Ceph without applying the file system tricks of Dovecot Pro?

.The answer is somehow unsatisfactory.

We can only ignore the lib-index, the central part of Dovecot, completely and implement an analogous solution that stores its data in Ceph.

Why?

* The lib-index does not have a plugin concept and can only be completely replaced.
* The lib-index is always used directly without any indirection. So we can not plug a new lib-index from the outside, but we must intervene in the code or the building process of Dovecot if we do not want to touch the places of use of the lib-index in the lib-storage.

The best cut is likely to be achieved by using the internal, plugin-enabled interface of lib-storage. Then you have to rewrite the lib-index and the lib-storage internal parts of the index & cache management.

That would mean We need to rewrite a lot of stable code and keep to the API of Dovecot at the same time

Suppose we take this lot on us. How could this work well with Ceph?

The data seams well mappable on key / values with prefix iterators. This paradigm is also found in the omap of the Ceph objects. Since Dovecot makes the access to the data with a file system in mind, it could possibly not be enough, always directly access the omap. A local cache for the index data seams to be advisable.

RocksDB with CephFS:: As a fast embedded key / value memory, RocksDB comes into question. The expressive possibilities are analogous to the abilities of the omap. In fact, the omap has even been realized in some Ceph versions. We would manage a RocksDB per user in shared storage in the first approach. The data areas are separated by prefixes. Many mechanisms implemented in Dovecot "by hand", such as the transaction logs, are already available at RocksDB. So we would have got rid of the lib-index, but we would not have really come closer to our goal.

.Hybrid Storage librmb & RocksDB index management with optional mail cache
[ditaa, "dovecot-ceph-hybrid-rocksdb", round-corners=true, separation=false]
....
+---------------------------------------------------------------------------+
|                          IMAP4/POP3/LMTP process                          |
++-------------------------------------------------------------------------++
 |                              storage API                                |
 ++-----------------------------------------------------------------------++
  |                             rbox backend                         cF84 |
  +-----------------------------------------------------------------------+
  |                               librmb                             cF84 |
  ++-------------+-------------------+-------------------+---------------++
   |librados c9DE|                   :                   | RocksDB   cYEL|
   +-----+-------+                   :                   +---------------+
         |                           :                   | env_posix cYEL|
         |                           :                   +-------+-------+
         |                           :                           |
         |                           :                           v
         |                           :                   +---------------+
         |       RFC 5322 Mails      :                   | Linux Kernel  |
         |                           :                   +---------------+
         |                           :                   | libcephfs cDDD|
         |                           :                   +---------------+
         |                           :                   | librados c9DE |
         |                           :                   +---------------+
         |                           :                           |
         v                           v                           v
 +-------------------------+ +----------------+         +-----------------+
 | +-+ +-+ +-+        Ceph | :  local storage |         |  CephFS for  {s}|
 | +-+ +-+ +-+ {s}         | |    RFC5322 cGRE|         |index & metadata |
 | RFC5322 objects     cF87| |     cache   {s}|         |             cF87|
 +-------------------------+ +----------------+         +-----------------+
....

For the shared storage we would use, like in the hybrid model, CephFS.

For the shifting of the index data to Ceph without using CephFS, we have the following solution ideas:

RocksDB with RADOS backend:: For the RocksDB exists a storage backend, which, instead of using a local file system, works directly with the librados on Ceph objects. The backend uses Ceph objects as file substitutes and maps the minimum necessary directory operations to omap key / values. A RocksDB in Ceph objects can only be used exclusively and must be protected by locks. This should not be a disadvantage in the Dovecot scenario.

.RocksDB with RADOS backend
[ditaa, "dovecot-ceph-rocksdb-rados-backend", round-corners=true, separation=false]
....
+---------------------------------------------------------------------------+
|                          IMAP4/POP3/LMTP process                          |
++-------------------------------------------------------------------------++
 |                              storage API                                |
 ++-----------------------------------------------------------------------++
  |                            rbox backend                          cF84 |
  +-----------------------------------------------------------------------+
  |                               librmb                             cF84 |
  |                                                +--------------------+-+
  |                                                |     RocksDB    cYEL|
  |                                                +-------------------++
  |                                                |  env_librados cYEL|
  +----------+-----------+-------------------------+------------------++
             :           |                 librados               c9DE|
             |           +--------------+----------------------+------+
             |                          |                      |
             |      RFC5322 Mails       |                      |
             v                          v                      v
     +---------------+         +--------------------------------------+
     :{s}            |         |                 Ceph                 |
     |    RFC5322    |         | +-+ +-+ +-+              +-+ +-+ +-+ |
     |     cache     |         | +-+ +-+ +-+   {s} cF87   +-+ +-+ +-+ |
     | local storage |         |   RFC5322                  RocksDB   |
     +---------------+         +--------------------------------------+
....

Local RocksDB with Sync to Ceph:: All data is managed in a local RocksDB. Their data are regularly replicated in the background into the omap key / values of Ceph objects. If the local RocksDB is missing, it is generated and initialized with the data from the omap. Through the corresponding functions of RocksDB, all changes are replicated from the RocksDB transaction log in Ceph omap. Competing changes can be detected through Ceph Watches.

.Local RocksDB with omap replication
[ditaa, "dovecot-ceph-rocksdb-omap-replication", round-corners=true, separation=false]
....
+-------------------------------------------------------------------------------+
|                             IMAP4/POP3/LMTP process                           |
++-----------------------------------------------------------------------------++
 |                                 storage API                                 |
 ++---------------------------------------------------------------------------++
  |                               rbox backend                           cF84 |
  +---------------------------------------------------------------------------+
  |                                  librmb                              cF84 |
  +--------+-----------+------------------+----+---------------+             ++
           |           |    RocksDB   cYEL|<-->|index sync cF84|             |
           |           ++----------------++    ++--------------+------------++
           |            |  env_posix cYEL|      |    librados           c9DE|
           |            +-------+--------+      +---+----------------+------+
           |                    |                   ^                |
        RFC5322           index & cache             |             RFC5322
         mails                  |                 index            mails
           :                    |                 omap               |
           :                    |                   |                |
           v                    v                   v                v
    +---------------+   +----------------+   +------------------------------+
    :{s}        cGRE|   |{s}         cGRE|   |             Ceph             |
    |    RFC5322    |   | index cache &  |   | +-+ +-+ +-+      +-+ +-+ +-+ |
    |     cache     |   |     cache      |   | +-+ +-+ +-+      +-+ +-+ +-+ |
    | local storage |   | local storage  |   |   index   {s} cF87 RFC5322   |
    +---------------+   +----------------+   +------------------------------+
....


Ceph omap:: The index data are stored directly in the ceph objects via the librados omap API. Contrary to the other suggestions we come here without a RocksDB. However, the operations on the key / values are expected to be much more fragmentary, which will lead to a higher access rate on Ceph. This solution will only work if access to the omap key / values is possible with really low latency. The RFC5322 mails are still temporarily stored locally.

.Pure RADOS backend
[ditaa, "dovecot-ceph-librmb-backend", round-corners=true, separation=false]
....
+---------------------------------------------------------------------------+
|                          IMAP4/POP3/LMTP process                          |
++-------------------------------------------------------------------------++
 |                              storage API                                |
 ++-----------------------------------------------------------------------++
  |                             rbox backend                         cF84 |
  ++---------------------------------------------------------------------++
   |                               librmb                                |
   |                                                                cF84 |
   +---------+-----------+----------------------------------------------++
             :           |                 librados                 c9DE|
             :           +--------------+----------------------+--------+
             :                          |                      |
             :      RFC5322 Mails       |                      |
             v                          v                      v
     +---------------+         +--------------------------------------+
     :{s}        cGRE|         |                 Ceph                 |
     |    RFC5322    |         | +-+ +-+ +-+              +-+ +-+ +-+ |
     |     cache     |         | +-+ +-+ +-+   {s} cF87   +-+ +-+ +-+ |
     | local storage |         |   RFC5322                  Index     |
     +---------------+         +--------------------------------------+
....

With all the solutions outlined above, Ceph-based solutions for the dictionaries and FTS must also be implemented in order to store the persistent portions of the mailboxes completely in Ceph.

==== How much is the price in lines of code?

Assuming that we need to create function-equivalent solutions for

* lib-index
* lib-storage/list
* lib-storage/index
* lib-storage/dbox-common
* lib-storage/dbox-multi

we would have to replace about 12% of the Dovecot code, which is about 370000 lines of C/C++ code. If you add

* fts-squat
* rados-dict
* fs-cache

you would end with an estimated 15%.

==== If all this does not work...

If all this does not work, we must accept that the solution from Dovecot Pro is superior in the context of the present Dovecot implementation.

If this solution were to be reproduced, a huge portion of the Dovecot Pro solution would have to be reprogrammed. The access to Ceph would take place directly via the lib-rados without the RGW.

.Dovecot Pro obox reimplementaion using librados
[ditaa, "dovecot-pro-ceph", round-corners=true, separation=false]
....
+---------------------------------------------------------------------------+
|                          IMAP4/POP3/LMTP process                          |
++-------------------------------------------------------------------------++
 |                              storage API                                |
 ++-----------------------------------------------------------------------++
  |                             obox backend                          cF84|
  +---+-------------------------------------+-------------------------+---+
      |                                     |                         |
      |                                     v                         |
      |               +--------------------------------------------+  |
      |               |                 metacache              cF84|  |
RFC 5322 Mails        +----------------+--------------------------++  |write index & cache
      |                                |         librados     c9DE|   |to local storage
      |                                +--------------+-----------+   |
      |                                               |         ^     |
      |                                           sync local    |     |
      v                                        index&cache with |     |
 +------------------------------+                   Ceph        |     |
 |           fs API             |                     |         |     v
 ++----------------------------++   +----------+      |      +--+--------+
  |     fs fscache backend cF84+--->| local    |      |      |   local   |
  +----------------------------+    | storage  |      |      |  storage  |
  |      fs RADOS backend  cF84|    | mail cGRE|      |      | cGRE   {s}|
  ++--------------------------++    | cache {s}|      |      +-----------+
   |         librados     c9DE|     +----------+      |
   +---+----------------------+                       |
       |                                              |
       v                                              v
 +------------------------------------------------------------+
 | +-+ +-+ +-+             object store           +-+ +-+ +-+ |
 | +-+ +-+ +-+                 {s} cF87           +-+ +-+ +-+ |
 | RFC5322 objects                        index&cache bundles |
 +------------------------------------------------------------+
....

== The discussion has been opened

Since the source code of the lib-index is only weakly documented, a lot of research work will be necessary to reproduce the existing functionality in the above variants. Whether the desired artifact of a general-use librmb arises, I dare to doubt. We will have to stay too close to Dovecot's code and APIs to create a truly stand-alone solution.

The above description has shown various approaches which approach the goal with different results and effort. It is now necessary to discuss and determine what the first steps will be.

=== Milestones

The following milestones were defined in the last meeting:

Milestone 1:: lib-dict plugin using one Ceph object per dictionary. Key / values are stored in omap.
_optional_ Milestone 1b:: lib-dict plugin using one Ceph object per key / value

After that, the areas that bring the greatest advantage in storage aspects should be a priority. This will mean to start with the preferred hybrid solution:

Milestone 2:: rbox storage plugin with lib-index for index & cache on CephFS and a librmb for storing mails in Ceph objects
Milestone 3:: Extend librmb or rbox storage plugin with a local cache for RFC5322 data if necessary

Only if required and with good reason, the more complex alternative models should be started.

Milestone 5:: rbox storage plugin using librmb for storing mails in Ceph and a key / value based index data management. The technology has yet to be defined. The appropriate solutions should be evaluated for this.

<<<
[appendix]
== Dictionary Design

The Dovecot dictionaries are a good candidate to be implemented using the Ceph omap key/value store. A RADOS dict driver will be a very thin layer above librados.

.Ceph omap
****
Each Ceph object consists of three different data storage areas, all of which are 100% optional: the "bundle of bits" object data, the object xattrs, and the object omap key-value store. An object's omap is not physically located in the object's storage, but its precise implementation is invisible and immaterial to RADOS applications. The omap keys and values as the other parts of an object are addressed by pool, namespace and object id (oid).
****

For each dictionary a name, a Ceph pool and a Ceph namespace are configured. Each dictionary consists of two Ceph objects, one for _shared/_ and another for _priv/_ keys. The resulting oid will build differently for the two namespaces mentioned above:

* `dict.shared.<dictionary name>`
* `dict.<username>.<dictionary name>`

All dictionary keys are mapped to omap keys.

The dictionary transactions will be mapped to Ceph operations. For the _atomic_inc_ a OSD class method must be implemented. OSD class methods can be called in an operation context. If the transaction contains keys from the _shared/_ and _priv/_ namespace, two operations will be used. This is possible because the already existing Dovecot dictionary implementations do no complete rollback is one of the operations fails. So using two operations would not harm the consistency expectations of Dovecot.

Another solution would be, to use one object per dictionary key. This would allow us to read or write the data in parallel.

[appendix]
== Implementation Languages

* The entire Dovecot is written in C. For the Lucene FTS plugin a small wrapper code in {cpp} is included. Fortunately, and this shows that {cpp} can be well integrated.

* The librados is written in {cpp} . A C-wrapper is available. Since {cpp} is well integrated into Dovecot, it is recommended to develop the Dovecot plugins in {cpp} and connect it to the C API of Dovecot via a storage module as a wrapper.

* RocksDB is written in {cpp}. A C-wrapper is available. Since {cpp} is well integrated into Dovecot, it is also useful here to develop the Dovecot plugins in {cpp} and connect them to the C API from Dovecot via a wrapper.

[appendix]
== Performance Insurance
To be able to judge the impacts of design or implementation decisions and changes it is necessary to know the anatomy of typical IMAP, LMTP or LDA operations.

=== Measure
We have to define a set of typical mail operations and record the calls to the backend that are performed during their execution. Typical operations might be

* New mail delivery
* INBOX check
* Read mail
* Delete Mail
* Search mail
* Set flags

=== Scale
To get am impression of the change rate for the whole system, mailboxes and mails we should analyse a real large scale Dovecot installation. Key performance indicators are

* Number of incoming mails overall (or for one storage node) per time unit
* Number of deleted mails overall (or for one storage node) per time unit
* Number of incoming mails per mailbox per time unit
* Number of deleted mails per mailbox per time unit
* Number of read mails per mailbox per time unit
* Number of metadata/flag changes per mailbox per time unit
* Size of mails
* Size of attachments
* MIME structures

=== Test
Using the above data we should build examples mailboxes, functional tests and load tests using eg. http://www.imapwiki.org/ImapTest[ImapTest] or a similar tool.


[appendix]
== Further Reading

=== Mailbox Formats

==== Supported by Dovecot
* http://wiki.dovecot.org/MailboxFormat[Mailbox Formats]
** http://wiki.dovecot.org/MailboxFormat/Cydir[Cydir]: _Its code is small and simple, so it can also act as an example for writing new mail storage backends._
** http://wiki.dovecot.org/MailboxFormat/imapc[imapc]
** http://wiki.dovecot.org/MailboxFormat/Maildir[Maildir]
** http://wiki.dovecot.org/MailboxFormat/mbox[mbox]
** http://wiki.dovecot.org/MailboxFormat/dbox[dbox]
** obox
*** https://oxpedia.org/wiki/index.php?title=Dovecot:Main_Page_Dovecot#dovecotbackendconf[Dovecot Pro Backend Configuration]
*** https://software.open-xchange.com/products/dovecot/doc/Dovecot-EE-repository-and-object-storage-installation-manual_v1.9.pdf[Object Storage and Enterprise Repository Installation manual]
*** https://www.heinlein-support.de/sites/default/files/dovecot_recent_and_future_development.pdf[Presentation by Timo ]
*** http://www.admin-magazine.com/Archive/2015/26/Scalable-mail-storage-with-Dovecot-and-Amazon-S3[Scalable mail storage with Dovecot and Amazon S3]

==== Supported by Other
* http://www.washington.edu/imap/documentation/mixfmt.txt.html[UW IMAP Server MIX]

=== Dovecot Internals
* http://wiki.dovecot.org/Design/Storage/Plugins[Storage Plugins]
** https://github.com/posteo/scrambler-plugin[Dovecot encryption plugin]
* http://wiki2.dovecot.org/Dictionary[Dictionary]
* http://wiki2.dovecot.org/Design/Indexes[Indexes]

=== Other Solutions
* https://lists.andrew.cmu.edu/pipermail/cyrus-devel/2015-September/003468.html[OpenIO: open source object storage for Cyrus]
* https://sys4.de/de/blog/2013/06/06/postfix-dovecot-ceph-cluster-storage/[CEPH Cluster für Dovecot MDBOX und Postfix]

=== API Inspirations
* http://www.dovecot.fi/dovecot-launches-a-gmail-api-compatible-rest-api-to-enable-third-party-developers-to-join-its-global-email-ecosystem/[Welcome to Dovecot: home to the world's most successful open source IMAP software]
* https://developers.google.com/gmail/api/guides/[Gmail API]
* https://msdn.microsoft.com/office/office365/api/mail-rest-operations[Outlook Mail REST API reference]

=== Project References

* https://trello.com/c/ao7wxVE5/120-dovecot-libradosmail[Sage Weil] im Ceph Backlog

<<<
[appendix]
== About the Project
This proposal has already been reported as Ceph project http://tracker.ceph.com/issues/12430[FEATURE #12430] . This project is supported by _Sage Weil_, who is the head Ceph developer and creator.


[appendix]
== About the Authors
*{author}* from https://tallence.com[Tallence Consulting] is working on the design and development of Deutsche Telekom email infrastructure since 2001 and a lot of other Deutsche Telekom backend services since 1998. The Deutsche Telekom email servers are Dovecot based since 2011.

You can contact {author} at {email}.

<<<
[appendix]
== Versions
[grid="cols", options="header", cols="1,3,1,1"]
|===
| Version | Description | Date | Author

| 1.0.0
| Initial release
| 2016-12-14
| Peter Mauritius

| 1.1.0
| Documentation of various design alternatives. Decision still open.
| 2017-02-13
| Peter Mauritius

| 1.2.0
| Stronger focus on librmb. Decision for index storage still open.
| 2017-02-28
| Peter Mauritius

| 1.3.0
| Hybrid Model with CephFS is preferred now.
| 2017-03-27
| Peter Mauritius

|===
